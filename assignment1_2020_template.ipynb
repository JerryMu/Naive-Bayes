{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### The University of Melbourne, School of Computing and Information Systems\n",
    "# COMP30027 Machine Learning, 2020 Semester 1\n",
    "\n",
    "## Assignment 1: Naive Bayes Classifiers\n",
    "\n",
    "###### Submission deadline: 7 pm, Monday 20 Apr 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student Name(s):**    `PLEASE ENTER YOUR NAME(S) HERE`\n",
    "\n",
    "**Student ID(s):**     `PLEASE ENTER YOUR ID(S) HERE`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iPython notebook is a template which you will use for your Assignment 1 submission.\n",
    "\n",
    "Marking will be applied on the four functions that are defined in this notebook, and to your responses to the questions at the end of this notebook (Submitted in a separate PDF file).\n",
    "\n",
    "**NOTE: YOU SHOULD ADD YOUR RESULTS, DIAGRAMS AND IMAGES FROM YOUR OBSERVATIONS IN THIS FILE TO YOUR REPORT (the PDF file).**\n",
    "\n",
    "You may change the prototypes of these functions, and you may write other functions, according to your requirements. We would appreciate it if the required functions were prominent/easy to find.\n",
    "\n",
    "**Adding proper comments to your code is MANDATORY. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy import stats\n",
    "from copy import copy, deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    read all datasets \n",
    "adult = pd.read_csv(\"datasets/adult.data\", header = None)\n",
    "bank = pd.read_csv(\"datasets/bank.data\", header = None)\n",
    "breast_cancer_wisconsin = pd.read_csv(\"datasets/breast-cancer-wisconsin.data\", header = None)\n",
    "car = pd.read_csv(\"datasets/car.data\", header = None)\n",
    "lymphography = pd.read_csv(\"datasets/lymphography.data\", header = None)\n",
    "mushroom = pd.read_csv(\"datasets/mushroom.data\", header = None)\n",
    "nursery = pd.read_csv(\"datasets/nursery.data\", header = None)\n",
    "somerville = pd.read_csv(\"datasets/somerville.data\", header = None)\n",
    "university = pd.read_csv(\"datasets/university.data\", header = None)\n",
    "wdbc = pd.read_csv(\"datasets/wdbc.data\", header = None)\n",
    "wine = pd.read_csv(\"datasets/wine.data\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Helper function that delete all missing values\n",
    "def delete_missing_value(raw_dataset, missing_values):\n",
    "    rows = set(raw_dataset[raw_dataset.values == missing_values].index)\n",
    "    data = raw_dataset.drop(index = rows)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    This function should prepare the data by reading it from a file and\n",
    "#    converting it into a useful format for training and testing\n",
    "\n",
    "def preprocess(df,missing_v):\n",
    "#     train=df.sample(frac=0.8,random_state=200)\n",
    "#     test=df.drop(train.index)\n",
    "#  Delete all missing values\n",
    "    train = delete_missing_value(df,missing_v)\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30162, 15)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                 1       2          3   4                   5   \\\n",
       "0  39         State-gov   77516  Bachelors  13       Never-married   \n",
       "1  50  Self-emp-not-inc   83311  Bachelors  13  Married-civ-spouse   \n",
       "2  38           Private  215646    HS-grad   9            Divorced   \n",
       "\n",
       "                  6              7      8     9     10  11  12             13  \\\n",
       "0       Adm-clerical  Not-in-family  White  Male  2174   0  40  United-States   \n",
       "1    Exec-managerial        Husband  White  Male     0   0  13  United-States   \n",
       "2  Handlers-cleaners  Not-in-family  White  Male     0   0  40  United-States   \n",
       "\n",
       "      14  \n",
       "0  <=50K  \n",
       "1  <=50K  \n",
       "2  <=50K  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0      int64\n",
       "1     object\n",
       "2      int64\n",
       "3     object\n",
       "4      int64\n",
       "5     object\n",
       "6     object\n",
       "7     object\n",
       "8     object\n",
       "9     object\n",
       "10     int64\n",
       "11     int64\n",
       "12     int64\n",
       "13    object\n",
       "14    object\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1.657715005636231e-05"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Global Variable\n",
    "\n",
    "feature_number = 14\n",
    "class_column = 14\n",
    "missing_values = \"?\"\n",
    "EPSILON = 1/(2*train_data.shape[0])\n",
    "\n",
    "# 0 represents nomianl, 1 represents ordinal, 2 represents continuous\n",
    "feature_types = [2, 0, 2, 1, 1, 0, 0, 0, 0, 0, 2, 2, 2, 0]\n",
    "\n",
    "#  Enter dataset here\n",
    "train_data = preprocess(adult, missing_values)\n",
    "\n",
    "# Show some useful information about the data\n",
    "display(train_data.shape)\n",
    "display(train_data.head(3))\n",
    "display(train_data.dtypes)\n",
    "display(EPSILON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate prior\n",
    "def find_nominal_prob(column):\n",
    "    prior = {}\n",
    "    total_number = len(column)\n",
    "# All possible case\n",
    "    classes = column.value_counts()\n",
    "    for key in classes.keys():\n",
    "        prior[key] = math.log2(classes[key]/total_number)\n",
    "    return prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nominal_llh(attributes, llh_dict, data, feature_row):\n",
    "    all_feature = data[feature_row].value_counts()\n",
    "    for attribute in attributes:\n",
    "        feature_dict = {feature_row:{}}\n",
    "        subset = data[data[class_column] == attribute][feature_row]\n",
    "        feature_dict[feature_row] = find_nominal_prob(subset)\n",
    "        for check_feature in all_feature.keys():\n",
    "            if check_feature not in feature_dict[feature_row].keys():\n",
    "                feature_dict[feature_row][check_feature] = np.log2(EPSILON)\n",
    "        if(llh_dict[attribute]):\n",
    "            llh_dict[attribute].update(feature_dict)\n",
    "        else:\n",
    "            llh_dict[attribute] = feature_dict\n",
    "            \n",
    "    return llh_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_numerical_llh(attributes, llh_dict, data, feature):\n",
    "#    Save mean and Std as llh\n",
    "    for attribute in attributes:\n",
    "        feature_dict = {feature:{}}\n",
    "        subset = subset = data[data[class_column] == attribute][feature]\n",
    "        mean = np.mean(subset)\n",
    "        std = np.std(subset)\n",
    "        feature_dict[feature][\"mean\"] = mean\n",
    "        feature_dict[feature][\"std\"] = std\n",
    "        if(llh_dict[attribute]):\n",
    "            llh_dict[attribute].update(feature_dict)\n",
    "        else:\n",
    "            llh_dict[attribute] = feature_dict\n",
    "    return llh_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should calculat prior probabilities and likelihoods from the training data and using\n",
    "# them to build a naive Bayes model\n",
    "\n",
    "def train(data):\n",
    "    attributes = find_nominal_prob(data[class_column])\n",
    "    prior = find_nominal_prob(data[class_column])\n",
    "    llh_dict = dict.fromkeys(attributes)\n",
    "    for i in range(feature_number):\n",
    "        if(feature_types[i] == 2):\n",
    "            find_numerical_llh(attributes, llh_dict, data, i)\n",
    "        else:\n",
    "            find_nominal_llh(attributes, llh_dict, data, i)\n",
    "            \n",
    "    return prior, llh_dict\n",
    "\n",
    "prior, llh_dict = train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<=50K'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This function should predict classes for new items in a test dataset (for the purposes of this assignment, you\n",
    "# can re-use the training data as a test set)\n",
    "\n",
    "def predict(instance, prior, llh_dict):\n",
    "    all_prob = {}\n",
    "    for attribute in prior.keys():\n",
    "        all_prob[attribute] = prior[attribute]\n",
    "        for i in range(feature_number):\n",
    "            prob = 0\n",
    "            if(feature_types[i] == 2):\n",
    "                mean = llh_dict[attribute][i][\"mean\"]\n",
    "                std = llh_dict[attribute][i][\"std\"]\n",
    "                prob = stats.norm.pdf(x=instance[i], loc=mean, scale=std)\n",
    "                if prob > 0.0:\n",
    "                    prob = np.log2(prob)\n",
    "                else:\n",
    "                    prob = np.log2(1 * 10 ** -8)\n",
    "            else:\n",
    "                prob = llh_dict[attribute][i][instance[i]]\n",
    "            all_prob[attribute] += prob\n",
    "    max_prob = -10000\n",
    "    max_key = \"\"\n",
    "    \n",
    "    for prob in all_prob.keys():\n",
    "        if(all_prob[prob] > max_prob):\n",
    "            max_prob = all_prob[prob]\n",
    "            max_key = prob\n",
    "    return max_key\n",
    "\n",
    "predict(train_data.iloc[100], prior, llh_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 3.1 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.821928254094556"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This function should evaliate the prediction performance by comparing your model’s class outputs to ground\n",
    "# truth labels\n",
    "\n",
    "def evaluate(data):\n",
    "    predict_list = []\n",
    "    correct = 0\n",
    "    total = data.shape[0]\n",
    "    for index, row in data.iterrows():\n",
    "        if row[14] == predict(row, prior, llh_dict):\n",
    "            correct += 1\n",
    "#         result_list = [index,row[14],predict(row, prior, llh_dict)]\n",
    "#         predict_list.append(result_list)\n",
    "    correct_rate = correct/total\n",
    "    return correct_rate\n",
    "%time\n",
    "evaluate(train_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions \n",
    "\n",
    "\n",
    "If you are in a group of 1, you will respond to question (1), and **one** other of your choosing (two responses in total).\n",
    "\n",
    "If you are in a group of 2, you will respond to question (1) and question (2), and **two** others of your choosing (four responses in total). \n",
    "\n",
    "A response to a question should take about 100–250 words, and make reference to the data wherever possible.\n",
    "\n",
    "#### NOTE: you may develope codes or functions in respond to the question, but your formal answer should be added to a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1\n",
    "Try discretising the numeric attributes in these datasets and treating them as discrete variables in the na¨ıve Bayes classifier. You can use a discretisation method of your choice and group the numeric values into any number of levels (but around 3 to 5 levels would probably be a good starting point). Does discretizing the variables improve classification performance, compared to the Gaussian na¨ıve Bayes approach? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#给原有的数据做一个备份\n",
    "backup = train_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19212    90\n",
       "5104     90\n",
       "11996    90\n",
       "1935     90\n",
       "5406     90\n",
       "20610    90\n",
       "24043    90\n",
       "28463    90\n",
       "18725    90\n",
       "15356    90\n",
       "10210    90\n",
       "32277    90\n",
       "222      90\n",
       "19747    90\n",
       "18832    90\n",
       "5272     90\n",
       "31030    90\n",
       "6232     90\n",
       "6624     90\n",
       "8973     90\n",
       "14159    90\n",
       "12975    90\n",
       "2891     90\n",
       "1040     90\n",
       "32367    90\n",
       "11512    90\n",
       "15892    90\n",
       "4070     90\n",
       "10545    90\n",
       "5370     90\n",
       "         ..\n",
       "29795    17\n",
       "11571    17\n",
       "21105    17\n",
       "26525    17\n",
       "29982    17\n",
       "2381     17\n",
       "21196    17\n",
       "24535    17\n",
       "8094     17\n",
       "28167    17\n",
       "7360     17\n",
       "5396     17\n",
       "26640    17\n",
       "17486    17\n",
       "16131    17\n",
       "26590    17\n",
       "19173    17\n",
       "17474    17\n",
       "17383    17\n",
       "12971    17\n",
       "30328    17\n",
       "5367     17\n",
       "29130    17\n",
       "30026    17\n",
       "27773    17\n",
       "23881    17\n",
       "6820     17\n",
       "10131    17\n",
       "10128    17\n",
       "11437    17\n",
       "Name: 0, Length: 30000, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Change feature values\n",
    "feature = train_data.iloc[:,0]\n",
    "series = feature.copy()\n",
    "series.sort_values(ascending = False)[0:30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a     30000\n",
       "90       35\n",
       "76       22\n",
       "77       20\n",
       "80       16\n",
       "79       15\n",
       "78       14\n",
       "81       13\n",
       "84        8\n",
       "82        7\n",
       "83        5\n",
       "88        3\n",
       "85        3\n",
       "86        1\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#找到这30000个数据,并把其的index换为临时index\n",
    "series[feature.sort_values()[0:30000].index] = 'a'\n",
    "series.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-27-3c95b77d256c>, line 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-27-3c95b77d256c>\"\u001b[0;36m, line \u001b[0;32m28\u001b[0m\n\u001b[0;31m    copy_feature[feature<=np.quantile(feature,1-quantile*k)] = (‘level'+str(levels-k))\u001b[0m\n\u001b[0m                                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "methods = ['width','freq','quantile']\n",
    "data_type=[2,0,2,1,1,0,0,0,0,0,2,2,2,0] #data = backup.copy()\n",
    "def discretise(data,data_type,levels,by):\n",
    "    '''\n",
    "    given a dataset and its corresponding data types, discretise all its numerical features into discre te features\n",
    "by must be chosen from one of 'width' or 'freq', standing for equal-width or equal-frequency\n",
    "levels takes a postivie integer, deciding the resultant number of feature levels after discretisati on\n",
    "    '''\n",
    "    copy_data = data.copy()\n",
    "    discrete_datatype = data_type.copy()\n",
    "    \n",
    "    for i in range(len(data_type)):\n",
    "        if data_type[i] == 2: # continuous \n",
    "            feature = data.iloc[:,i]\n",
    "            copy_feature = feature.copy()\n",
    "        # equal width\n",
    "        if by=='width': \n",
    "        # calculate width of each level \n",
    "            maximum = np.max(feature)\n",
    "            minimum = np.min(feature)\n",
    "            width = (maximum-minimum)/levels # width of each level \n",
    "            for j in range(levels): \n",
    "                copy_feature[feature<=(maximum-width*j)] = ('level'+str(levels-j)) \n",
    "        # equal quantile \n",
    "        elif by=='quantile': \n",
    "            quantile = 1/levels\n",
    "            for k in range(levels): \n",
    "                copy_feature[feature<=np.quantile(feature,1-quantile*k)] = (‘level'+str(levels-k))\n",
    "        # equal frequency \n",
    "        elif by==‘freq':\n",
    "            frequency = feature.shape[0]/levels # frequency of each level \n",
    "            for m in range(levels): \n",
    "                lower = int(frequency*m) # lower bound of index, so has to be integer\n",
    "                upper = int(frequency*(m+1)) # upper bound of index, so has to be integer \n",
    "                copy_feature[feature.sort_values()[lower:upper].index] = ('level' + str(levels-m))\n",
    "\n",
    "            copy_data.iloc[:,i] = copy_feature \n",
    "            discrete_datatype[i] = 1\n",
    "    return copy_data, discrete_datatype \n",
    "\n",
    "width_data,discrete_datatype = discretise(data,data_type,levels=5,by='width')\n",
    "qtile_data,discrete_datatype = discretise(data,data_type,levels=5,by='quantile')\n",
    "freq_data,discrete_datatype = discretise(data,data_type,levels=5,by='freq')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-662a59a2db7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# check one feature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# check one feature\n",
    "data.iloc[:,10].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width_data.iloc[:,10].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtile_data.iloc[:,10].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_data.iloc[:,10].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "width_model = train(width_data,label_index,labels,discrete_datatype,smoothing='epsilon',k=0)\n",
    "width_preds = predict(width_data,label_index,discrete_datatype,width_model)\n",
    "width_eval = evaluate(width_preds,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtile_model = train(qtile_data,label_index,labels,discrete_datatype,smoothing='epsilon',k=0)\n",
    "qtile_preds = predict(qtile_data,label_index,discrete_datatype,qtile_model)\n",
    "qtile_eval = evaluate(qtile_preds,labels)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_model = train(freq_data,label_index,labels,discrete_datatype,smoothing='epsilon',k=0)\n",
    "freq_preds = predict(freq_data,label_index,discrete_datatype,freq_model)\n",
    "freq_eval = evaluate(freq_preds,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2\n",
    "Implement a baseline model (e.g., random or 0R) and compare the performance of the na¨ıve Bayes classifier to this baseline on multiple datasets. Discuss why the baseline performance varies across datasets, and to what extent the na¨ıve Bayes classifier improves on the baseline performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import randint from random import seed\n",
    "def random_pred(data,label_index): \n",
    "    predictions = []\n",
    "    for i in range(data.shape[0]):\n",
    "        instance = data.iloc[i,:]\n",
    "        true_label = instance[label_index]\n",
    "        seed(i) # different seed for each instance\n",
    "        index = randint(0,len(labels)-1)\n",
    "        pred = labels[index]\n",
    "        predictions.append((i,true_label,pred))\n",
    "    return predictions\n",
    "random_predictions = random_pred(data,label_index)\n",
    "random_eval = evaluate(random_predictions,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_r_pred(data,label_index):\n",
    "    majority = data[label_index].value_counts().idxmax()\n",
    "    predictions = []\n",
    "    for i in range(data.shape[0]):\n",
    "        instance = data.iloc[i,:]\n",
    "        true_label = instance[label_index]\n",
    "        predictions.append((i,true_label,majority))\n",
    "    return predictions\n",
    "zero_r_predictions = zero_r_pred(data,label_index)\n",
    "zero_r_eval = evaluate(zero_r_predictions,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3\n",
    "Since it’s difficult to model the probabilities of ordinal data, ordinal attributes are often treated as either nominal variables or numeric variables. Compare these strategies on the ordinal datasets provided. Deterimine which approach gives higher classification accuracy and discuss why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4\n",
    "Evaluating the model on the same data that we use to train the model is considered to be a major mistake in Machine Learning. Implement a hold–out or cross–validation evaluation strategy (you should implement this yourself and do not simply call existing implementations from `scikit-learn`). How does your estimate of effectiveness change, compared to testing on the training data? Explain why. (The result might surprise you!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5\n",
    "Implement one of the advanced smoothing regimes (add-k, Good-Turing). Does changing the smoothing regime (or indeed, not smoothing at all) affect the effectiveness of the na¨ıve Bayes classifier? Explain why, or why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6\n",
    "The Gaussian na¨ıve Bayes classifier assumes that numeric attributes come from a Gaussian distribution. Is this assumption always true for the numeric attributes in these datasets? Identify some cases where the Gaussian assumption is violated and describe any evidence (or lack thereof) that this has some effect on the NB classifier’s predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
